\documentclass{article}
\usepackage[english]{babel}

\input{styles}

\title{
The Alan Turing Institute - 
Carbon Emissions from Computing}
\author{Rosie Wood}
\date{September 2025}
\affil{The Alan Turing Institute}

\begin{document}

\maketitle

\setcounter{tocdepth}{1}
\tableofcontents
\newpage

\section{Introduction}

Data centres accounted for around 1.5\% of the world’s electricity consumption and around 180 MtCO${_2}$e in 2024 \cite{iea_energy_ai}. 
This number is forecast to more than double by 2030, largely driven by AI\cite{iea_energy_ai}. The Turing's emissions are undoubtedly a \textit{tiny} subset of the above. However, as a first step towards engaging and understanding the emissions that come from computing, calculating our emissions will help us develop some knowledge in the area and understand potential areas where we could improve. 

The Turing's main compute resources are Microsoft Azure, UK High Performance Computing systems (HPCs) and GitHub Actions.
This report aims to estimate our emissions from these resources and ultimately help identify areas in which we could reduce these emissions.

Emissions are categorised as one of three scopes\cite{nationalgrid}:

\begin{itemize}
    \item Scope 1 covers emissions from sources that an organisation owns or controls directly.
    \item Scope 2 are emissions that a company causes indirectly and come from where the energy it purchases and uses is produced. Scope 2 emissions can be calculated in one of two ways:
    \begin{itemize}
        \item Market-based Scope 2 emissions reflect those from electricity that companies have purposefully chosen (i.e. if they have chosen a fully renewable tariff, market-based Scope 2 emissions will be zero).
        \item Location-based Scope 2 emissions reflect the average emissions intensity of grids on which energy consumption occurs.
    \end{itemize}
    \item Scope 3 encompasses emissions that are not produced by the company itself and are not the result of activities from assets owned or controlled by them, but by those that it’s indirectly responsible for up and down its value chain.
\end{itemize}

Emissions from compute broadly come from one of two sources: emissions from energy consumption when running the hardware (known as operational emissions) and embodied emissions from manufacturing, transporting and disposing of the hardware. For the compute provider (e.g. Azure or an HPC host organisation), operational emissions correspond to Scope 1 (on-site fuel use) and Scope 2 (purchased electricity), while embodied emissions fall under Scope 3 (upstream/downstream supply chain). For the Turing, all of our emissions from computing are classed as Scope 3 since we do not own the hardware we run on nor do we pay directly for the electricity that powers it. 

To calculate operational emissions, the carbon emissions from electricity use are calculated by multiplying energy consumption by the carbon intensity (CI) of that energy, where CI is a measure of the carbon emissions per kWh of electricity consumed. This is a relatively simple calculation and makes it fairly trivial to estimate operational emissions provided that energy consumption and CI are known. Embodied emissions, however, are far more complicated to calculate since they require upstream data from vendors, such as manufacturing and transportation emissions, and a life-cycle assessment.

To help put emissions from computing into perspective it can be helpful to compare against carbon emissions from other sources on similar scales:

 \begin{itemize}
     \item In the UK in 2022, the average person was responsible for approximately 5 tCO${_2}$e per year\cite{iea_uk_emissions}.
     \item A flight* from London (LHR) to:
     \begin{itemize}
         \item 1. Barcelona (BCN) emits approximately 0.292 tCO${_2}$e per person\cite{co2_flights},
         \item 2. New York (JFK) emits approximately 0.978 tCO${_2}$e per person\cite{co2_flights},
         \item 3. Melbourne (MEL) via Kuala Lumpar (KUL) emits approximately 3.4 tCO${_2}$e per person\cite{co2_flights}.
     \end{itemize}
 \end{itemize}

\footnote{*For details on the methodology used to calculate the emissions from the flights see the \href{https://www.myclimate.org/en/information/about-myclimate/downloads/flight-emission-calculator/}{Calculation Principles} page.}

\section{Microsoft Azure}

Microsoft Azure has its own carbon optimisation and emission tracking dashboard which make it possible to view emissions resulting from the Turing's Azure usage. 
Azure users can access this themselves and see data related to Azure subscriptions they are part of. An example screenshot is shown in Figure \ref{fig:azure_dashboard} (note: this shows data for different dates than discussed below).
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{azure_dashboard.png}
    \caption{Azure Carbon Optimisation dashboard}
    \label{fig:azure_dashboard}
\end{figure}

The Azure emissions dashboard breaks down emissions by month and by scope. 
Here, Scope 1 emissions refer to Microsoft's Scope 1 emissions (e.g. from their own power plants), Scope 2 emissions refer to Microsoft's Scope 2 emissions (e.g. from energy they buy from the grid) and Scope 3 emissions refer to Microsoft's Scope 3 emissions (e.g. from the hardware they purchase).

Figure \ref{fig:azure_monthly} shows the data for the past year (September 2024 - August 2025). From this, we see the majority of the Turing's Azure emissions come from Microsoft's Scope 3 emissions, with a small contribution from Microsoft's Scope 1 emissions (most likely back-up power generation). Over this period, the Turing's total emissions from Azure were just 2.9 tCO${_2}$e. 
This number is surprisingly low, likely because Scope 2 emissions, which we would expect to be the biggest contributor to Azure emissions, are reported as zero. 
This is because Microsoft only report market-based Scope 2 emissions and use renewable energy tariffs\cite{azure_docs_scope2}, hence, the CI of the energy is zero. Since there is no information on our energy usage, it is impossible to calculate location-based Scope 2 emissions.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{azure_monthly.png}
    \caption{Azure monthly emissions (September 2024 - August 2025)}
    \label{fig:azure_monthly}
\end{figure}

\section{HPC}

\subsection{Baskerville}

Baskerville is by far the most widely used HPC at the Turing due to our institutional allocation which makes it particularly easy for Turing projects to access and utilise Baskerville HPC. Consequently, it is also our biggest source of compute emissions.

Baskerville is hosted by the University of Birmingham which, based on information from the Baskerville team, uses a gas powered combined heat and power (CHP) plant to power it's campus. Consequently, Baskerville's operational emissions should be calculated using the CI of this CHP plant instead of that of the national grid. 
There is limited information about emissions from Birmingham's CHP plant, but the University of Edinburgh reported it's CHP plant emissions had a CI of 366 gCO${_2}$e kWh$^{-1}$\cite{edi_chp} - almost 3x higher than the UK national average of 124 gCO$_{2}$ kWh$^{-1}$\cite{average_ci_2024} in 2024. 
Noussan et al. also found similar numbers, suggesting annual average in the range 270–304 gCO$_{2}$ kWh$^{-1}$.\cite{NOUSSAN2024} For this work, I've used a value of 300 gCO${_2}$ kWh$^{-1}$ to represent the estimated CI of the University of Birmingham's CHP plant.

As Turing admins for Baskerville, we are able to access an admin portal to download usage data for the whole Turing across all time.
This data contains somewhat limited information for each job, but includes run times, requested resources and submission dates. 
This is enough information to be able to create a rough estimate for operational emissions originating from the Turing's usage of Baskerville.

The code in \href{https://github.com/rwood-97/hpc_co2/tree/main}{the hpc\_co2 repository} shows a simple way of calculating operational emissions from our Baskerville usage. 
This is done using the formula shown in Equation \eqref{eq:emission1} where $E_{gpu}$, $E_{cpu}$ and $E_{mem}$ are the energy used by the GPUs, CPUs and memory, respectively, $PUE$ is the power usage effectiveness of the HPC and $CI$ is the carbon intensity (CI) at the time of that energy usage. 
Energy is calculated using Equation \eqref{eq:energy1} where $E_{component}$ is the energy used by the component (GPU, CPU or memory), $t_{component}$ is the time that component is in use for and $P_{component}$ is the power draw of that component. 
For both GPU and CPU, power draw can be taken as the Thermal Design Power (TDP) of the component, as reported by the manufacturer, and for memory, power draw can be taken as a constant power draw of 0.3725 W GB$^{-1}$ of requested memory\cite{green_algorithms}.

\begin{equation}
    \mathit{Scope 2} = (E_{gpu} + E_{cpu} + E_{mem}) * \mathit{PUE} * \mathit{CI}
    \label{eq:emission1}
\end{equation}

\begin{equation}
    E_{component} = t_{component} * P_{component}
    \label{eq:energy1}
\end{equation}

Figure \ref{fig:baskerville_energy} shows the calculated energy usage of Turing's Baskerville jobs since mid-2021 (when the Turing first gained pilot access to Baskerville). 
Although these calculated energy values should only be taken as estimates due to large errors in the data, they are accurate enough to give a sense of our energy usage over the years. 
Errors originate from the following sources:
\begin{itemize}
    \item The energy values shown reflect the energy needed to operate GPUs, CPUs and memory, as well as the overheads of the Baskerville HPC facility (PUE). However, they do not take into account additional overheads, such as energy needed to operate interconnect switches, storage, coolant systems, etc. For ARCHER2, the UK National Supercomputing Service hosted by EPCC at the University of Edinburgh, these overheads account for around 15\% power draw\cite{archer2_scope2} and for Isambard-AI, an AI-optimised HPC and part of the UK's AI Research Resource, these overheads account for around 8\% of operational emissions\cite{grace_hpc}. This would mean the values are underestimating the energy requirements for running Baskerville.
    \item The energy values shown are calculated using the thermal design power (TDP) values provided by NVIDIA for their GPUs and therefore assume 100\% GPU power usage. However, GPU utilisation is often less than 100\% when running GPU jobs on HPC\cite{gpu_utilisation} and, in fact, according to the Baskerville team, some of the Baskerville GPUs are power capped. This would mean the values are overestimating the energy requirements when using Baskerville GPUs.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{baskerville_energy.png}
    \caption{Baskerville energy usage (August 2021 - August 2025)}
    \label{fig:baskerville_energy}
\end{figure}

As per Equation \eqref{eq:emission1}, energy values per job are multiplied by CI at runtime to calculate carbon emissions. 
Two approaches to this are: using the CI of the University of Birmingham's CHP plant (300 gCO$_{2}$ kWh$^{-1}$), or, using the CI of the national grid. 
For the latter, since CI values fluctuate throughout the day and throughout the year, different averaging approaches yield different emissions values. 
Three approaches to averaging CI are: 1. Taking a yearly average (e.g. in 2024 the UK average CI was 124 gCO$_{2}$ kWh$^{-1}$)\cite{average_ci_2024}, 2. Taking a daily average using the \href{https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0}{Carbon Intensity API}, and 3. Taking an hourly average, again using the \href{https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0}{Carbon Intensity API}.

The \href{https://github.com/rwood-97/hpc_co2/blob/ef0b52f26c255d6060262fe40ae09f21899418da/run_baskerville_emissions.ipynb}{run\_baskerville\_emissions notebook} shows how numbers vary using these different approaches. 
Using the CHP plant approach, with an average CI of 300 gCO$_{2}$ kWh$^{-1}$, the Turing's operational emissions over its entire use of Baskerville were 89.2 tCO${_2}$e and its operational emissions in the past year (September 2024 - August 2025) were 25.0 tCO${_2}$e. 
Using the national grid approach, with a yearly average CI of 124 gCO$_{2}$ kWh$^{-1}$, the Turing's operational emissions over its entire use of Baskerville were 36.9 tCO${_2}$e and its operational emissions in the past year (September 2024 - August 2025) were 10.3 tCO${_2}$e. This is around 2.4 times ($300/124$) less than using the CHP plant approach. 
Notably, even when using the national grid appoach, our operational emissions from Baskerville usage are significantly higher than from our entire Azure usage (operational plus embodied emissions).

Using the national grid approach with daily average CIs, the Turing's operational emissions over its entire use of Baskerville were 41.2 tCO${_2}$e and its operational emissions in the past year were 9.0 tCO${_2}$e. 
Using this approach, our overall emissions were higher but our emissions for the past year were lower. 
These variations are likely due to a UK trend of reducing CI over time as more renewables are added to the national grid, meaning historical CIs were higher than 124 gCO$_{2}$ kWh$^{-1}$ but this current year's average CI is lower.

Figure \ref{fig:baskerville_emissions_annually} compares the Turing's operational emissions from its use of Baskerville using the three approaches outlined above.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.75\linewidth]{baskerville_yearly_emissions.png}
    \caption{Baskerville emissions (August 2021 - August 2025)}
    \label{fig:baskerville_emissions_annually}
\end{figure}

A number of open source tools for measuring operational emissions from HPC are available on GitHub\cite{greenalgorithms4hpc, grace_hpc}. {GRACE-HPC}, developed by Elliot Ayliffe and the Bristol Centre for SuperComputing (BriCS) team\cite{grace_hpc}, is one example.
Generally, these tools use the slurm \href{https://slurm.schedmd.com/sacct.html}{sacct} command (or the non-slurm equivalent of this command if an alternative job scheduler is being used) to gather information about completed jobs and calculates their carbon emissions based on the CI at the jobs run time. 
However, due to privacy restrictions, Baskerville users are only able to see information about their own jobs using \texttt{sacct}. 
This is true even for Turing admins and so instead of running GRACE-HPC directly on Baskerville, \texttt{sacct} data for the Turing's usage of Baskerville was kindly provided directly by the Baskerville team. 
I made code changes (see \href{https://github.com/Elliot-Ayliffe/GRACE-HPC/pull/1}{\#1}, \href{https://github.com/Elliot-Ayliffe/GRACE-HPC/pull/2}{\#2} and \href{https://github.com/Elliot-Ayliffe/GRACE-HPC/pull/3}{\#3}) to ensure GRACE-HPC could use this pre-saved data as input and, to save time and reduce the number of API calls, added some caching and adjusted the CI averaging to use hourly averages versus getting exact CI values on a job-by-job basis.

Running GRACE-HPC on the provided data showed that the Turing's operational emissions were approximately 11.2 tCO${_2}$e from its usage of Baskerville in the past year (September 2024 - August 2025). 
Notably, this is around 25\% higher than calculated using daily average CIs, though it is unclear where these differences arise.

Figure \ref{fig:agg_hourly_run_time_baskerville_vs_ci} shows how the Turing's usage of Baskerville and the UK average CI vary throughout the day. From this, we see that on average the Turing has the highest number of jobs running between 11 am and 5 pm. Interestingly, when compared against the UK average CI, we see that our jobs are generally being run when CI is lowest.  

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{uk_ci_vs_usage.png}
    \caption{UK average carbon intensity and average number of Baskerville jobs running at each hour of the day (September 2024 - August 2025).}
    \label{fig:agg_hourly_run_time_baskerville_vs_ci}
\end{figure}


Interestingly, GRACE-HPC also highlights other data from our Baskerville usage. 
For example, the Turing ran 48,099 jobs over the course of the last year, summing to 138,073 GPUh. 
However, 48.9\% of those finished with a non-zero exit code. This indicates that these jobs did not run to completion. This could mean that jobs were interrupted due to an error or due to time constraints and therefore there is potential for lost results and wasted energy. According to GRACE-HPC, these jobs account for approximately 7.1 tCO${_2}$e - over half of the Turing's Baskerville emissions. Reducing the number of these jobs would significantly reduce our operational emissions from HPC use. This could be achieved by offering more training for new HPC users, guidance on best practices when submitting jobs to HPC and raising awareness of the emissions from failed jobs.

A full comparison between the four methods is shown in Table \ref{tab:op_emissions_comparison}. Clearly, the biggest influence on the operational emissions from our Baskerville usage is the provider of the electricity with less influence coming from the averaging method. Since the source of electricity for Baskerville HPC is a CHP plant, these numbers from the CHP plant approach are likely the best estimate of the four methods. Due to the errors in the energy estimates (mentioned above) and variations in the CI values, these operational emissions values should still only be taken as rough estimates.

\begin{table}
    \centering
\caption{Comparison of the different methods for calculating operational emission from HPC}
\label{tab:op_emissions_comparison}
    \begin{tabular}{rrrr}\toprule
        Approach&  Averaging method&  All time emissions /tCO${_2}$e& Year to date /tCO${_2}$e\\\midrule
         CHP plant &  Yearly&  89.2& 25.0\\
         National Grid&  Yearly&  36.9& 10.3\\
         National Grid&  Daily&  41.2& 9.0\\
         National Grid&  Hourly&  N/A& 11.2\\ \bottomrule
    \end{tabular}
    
    
\end{table}

\subsubsection{Emissions rate}

Since we don't necessarily want to reduce our usage of HPC, it's useful instead to think of emissions as a rate per functional unit.
For example, per GPUh, per useful output or, for AI workloads, per token.
In the past year, the Turing ran jobs summing to 138,073 GPUh and so our emissions rate per GPUh is 81.3 gCO${_2}$e per GPUh (using the GPUh and emissions values reported by GRACE-HPC). 
Methods of improving this rate would include making code more efficient by reducing idle times or using more efficient algorithms, ensuring requested resources are correct for the workload (e.g. ensuring the optimal number of GPUs/nodes/memory) and running jobs at times where CI is lower (also known as carbon aware scheduling). For the latter, this is only a viable approach if a) Baskerville GPU's are being used at less than 100\% capacity and b) it was a system-wide approach, i.e. there are no rebound effects.
A similar approach could be taken for AI workloads using rate per token. 

\subsubsection{Embodied emissions}

Recent work focused on estimating the embodied emissions of HPCs from the \href{https://top500.org/lists/top500/}{Top500 list}\cite{Rao_2025} estimated Baskerville's embodied emissions to be 381 tCO$_{2}$e. This value was calculated using EasyC\cite{easyc}, a tool which estimates embodied carbon emissions based on a only few simple parameters – the number of compute units, storage capacity and memory capacity.

To verify this number, we can compare against other HPCs for which in-depth embodied emissions analyses have been undertaken. For example, the embodied emissions from Isambard-AI (Phase 2), a GPU-based HPC, is estimated to be 8,184 tCO$_{2}$e\cite{isambard-ai_emissions}. Isambard-AI (Phase 2) is made up of 1320 nodes with GH200 chips. Each GH200 chip has 1 Grace CPU and 1 H100 GPU. There are 4 GH200 chips per node and 72 cores per GPU. Isambard-AI also has 1056 30.72 TB NVMe SSDs. In comparison, the Baskerville system is made up of 48 nodes with A100 GPUs and 2 nodes with H100 GPUs. For both A100 and H100 nodes, there are 4 GPUs per node and 36 cores per GPU. Baskerville also has 48 7.68 TB SSDs. From these system specifications, we see Isambard-AI has 26x more compute nodes than Baskerville. A naive estimate for the embodied emissions of Baskerville would therefore be around 310 tCO$_{2}$e ($8184/26$). However, this does not account for the different GPUs types, number of CPUs per node or SSD capacities. Nevertheless, this supports the idea that embodied emissions for Baskerville HPC should be on the order of 300 tCO$_{2}$e. 

Embodied emissions can be converted to embodied emissions per unit time such that embodied emissions can be apportioned to the different users of these HPC systems based upon their usage. For Isambard-AI, using an estimated 6 year service lifetime, embodied emissions are estimated to be 0.114 kgCO$_{2}$e per node hour\cite{isambard-ai_emissions}\cite{archer2_sustainability}. 
Since Baskerville has been running since 2021 and is due to be shut down in 2026, its service lifetime is 5 years. 
This gives an estimated embodied emissions per unit time of 0.174 kgCO$_{2}$e per node hour or 0.043 kgCO$_{2}$e per GPUh. 
Given our usage over the past year of 138,073 GPUh, the Turing's share of embodied emissions from our Baskerville usage is 6 tCO$_{2}$e in the past year.
Comparing this against the values in Table \ref{tab:op_emissions_comparison}, this suggests that operational emissions outweigh embodied emissions for Baskervillle HPC. This means that pushing for Baskerville to be powered by renewable electricity, or moving to a more green HPC, would be most effective method of reducing our emissions from compute. 

\section{GitHub actions}

Since GitHub uses Azure virtual machines (VMs) to run GitHub Actions workflows, and we know that Azure uses renewable electricity with a CI of zero, the most likely value for the Turing's operational emissions from GitHub actions is zero. Our embodied emissions are more difficult to estimate, but are likely to be low, since GitHub actions uses small VMs with only a few CPUs. Optimising our use of GitHub actions is therefore not likely to impact our overall emissions from compute.

\subsubsection{Market-based emissions}

For sake of argument, it is possible to calculate the Turing's emissions from GitHub Actions using the market-based approach. The \href{https://github.com/alan-turing-institute}{Turing's GitHub organisation} ran 3340 hours of GitHub Actions in the past 6 months (March 2025 - September 2025). To estimate the operation emissions from this work, we must know a) the energy consumption and b) CI of the energy consumed. 

 GitHub provide some information about the default runners used when a GitHub Actions workflow is triggered, including information about the number of CPUs and RAM on the VM\cite{github_runners_info}. This helps us understand what compute resources are being used and thus estimate their energy consumption. 
To understand the CI of the energy consumed, we need to know where this energy is coming from. I created a support request to GitHub to ask about the location of these runners and got the following response: ``Standard and larger GitHub-hosted runners are all hosted in North America (primarily in the United States, and occasionally Canada).`` 
Together, this information can be used to work out the approximate operational emissions from the Turing's use of GitHub actions using the Green Algorithms calculator\cite{green_algorithms}.

In the previous six months, a rough estimate for our operational emissions related to GitHub actions runners is 50 kgCO${_2}$e.
This was calculated using the Green Algorithms calculate and the following settings: 3340 hours of runtime, 4 CPUs, 7 GB memory and setting the Azure region as North Central US.
Assuming this value is representative, this equates to just 100 kgCO${_2}$e for the year - negligible compared to our Azure and HPC emissions and confirms that optimising GitHub Actions usage is unlikely to significantly affect our overall compute-related emissions.

\section{Conclusion}

This report explores three different sources of computing emissions from the Turing: Microsoft Azure, HPC and GitHub Actions. In the past year, the Turing's Azure usage was responsible for zero operational emissions (due to Microsoft's use of renewable energy) and 2.9 tCO$_{2}$e embodied emissions and, over the same timeframe, the Turing's Baskerville usage was responsible for 25 tCO$_{2}$e operational emissions and 6 tCO$_{2}$e embodied emissions. Emissions from our usage of GitHub actions can be considered as negligible. In total, we were responsible for around 34 tCO$_{2}$e emissions from compute in the past year. This is equivalent to just less than 7 average UK persons worth of emissions, or 10 flights from London to Melbourne (via Kuala Lumpur). More than anything, this demonstrates the huge emissions impact of flying.

Notably, the majority of the Turing's compute emissions came from Baskerville HPC, a GPU-based machine where the bulk of our workloads are AI-focused. This highlights the large impact of AI's emissions compared to other computing workloads and suggests contributing to research into greener AI technologies would be an effective route to reducing the Turing's emissions whilst also having a wider impact.

Reducing emissions from Baskerville is currently the largest opportunity for reducing our emissions, the easiest way of doing this would be to encourage the Baskerville team to put Baskerville on a renewable electricity tariff. 
An alternative approach, especially in light of Baskerville's end of life in 2026, would be to encourage Turing projects to move to a more green HPC. Other potential avenues for reducing the Turing's emissions from Baskerville usage include reducing the numbers of failed jobs, e.g by providing more training or guidance for HPC users, and focusing on improving the efficiency of our jobs, e.g. by optimising the compute resources requested or using more energy efficient algorithms.

Other potential sources of emissions which were not covered in this report due to lack of data, come from other UK HPC's which the Turing makes use of (e.g. Isambard-AI and Dawn), Turing-owned hardware (e.g. Pi's, laptops, commercial GPUs) and collaborators hardware.

\bibliography{bibliography}{}
\bibliographystyle{plain}

\end{document}


